---
title: "Problem Set 2: Bias, Variance, Cross-Validation"
author: "46496"
date: |
  | `r format(Sys.time(), '%d %B %Y')`
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
options(repr.plot.width=4, repr.plot.height=3)
library(ggplot2)

plot_decision_boundary <- function(train_x, train_y, pred_grid, grid) {
  cl <- ifelse(train_y == 1, "Pos", "Neg")
  # Data structure for plotting
  
  dataf <- data.frame(grid,
                      prob = as.numeric(pred_grid), #prob = attr(pred_grid, "prob"),
                      class = ifelse(pred_grid==2, "Pos", "Neg"))
  
  ## Plot decision boundary
  
  col <- c("#009E73", "#0072B2") # Hex color codes
  plot <- ggplot(dataf) +
    geom_raster(aes(x=x_1, y=x_2, fill=prob), alpha=.9,
                 data=dataf) +
    geom_point(aes(x=x_1, y=x_2, color=class),
               size=1,
               data=data.frame(x_1=train_x[,1], x_2=train_x[,2], class=cl)) +
    geom_point(aes(x=x_1, y=x_2),
               size=1, shape=1,
               data=data.frame(x_1=train_x[,1], x_2=train_x[,2], class=cl)) + 
    scale_colour_manual(values=col, name="Class") +
    scale_fill_gradientn(colors=col[c(2,1)], limits=c(0,1), guide = FALSE) + 
    xlab("Feature 1") + ylab("Feature 2")
  return(plot)
}
```

## 1. 10-fold CV using random dataset

Below is a dataset generated by adding gaussian noise to a pre-defined function. The true function is plotted in red.
```{r}

# because we are generating random data, set a random seed
set.seed(1)

# generate values in x spread evenly from 0 to 20
x <- seq(from=0, to=20, by=0.05)

# generate y according to the following known function of x
y <- 500 + 0.4 * (x-10)^3

# add random noise to y
noise <- rnorm(length(x), mean=10, sd=80)
noisy.y <- y + noise

# plot data
# red line for true underlying function generating y
{
  plot(x,noisy.y)
  lines(x, y, col='red')
}
```

a. With predictor `x` and outcome `noisy_y`, split the data into a training and test set.

```{r}
set.seed(1)
train_idx <- sample(1:length(x), size = floor(0.8 * length(x)))
train_x <- x[train_idx]
train_y <- noisy.y[train_idx]

test_x <- x[-train_idx]
test_y <- noisy.y[-train_idx]
```

b. Perform 10-fold CV for polynomials from degree 1 to 5 (use MSE as your error measure). This should be done from scratch using a for loop. *(Hint: It may be helpful to randomly permute and then split the training set from the previous section into 10 evenly sized parts. You may need an if statement to handle a potential problem in the last iteration of your loop.)*

```{r}
set.seed(1)
k <- 10
n <- length(train_x)
folds <- cut(seq(1, n), breaks = k, labels = FALSE)

# Calculate errors for polynomials of degrees 1 to 5
cv_errors <- matrix(0, nrow = 5, ncol = 1, dimnames = list(1:5, "CV_Error"))

for (degree in 1:5) {
  errors <- vector("numeric", k)
  
  for (i in 1:k) {
    fold_idx <- which(folds == i, arr.ind = TRUE)
    x_val <- train_x[fold_idx]
    y_val <- train_y[fold_idx]
    
    x_train_fold <- train_x[-fold_idx]
    y_train_fold <- train_y[-fold_idx]
    
    model <- lm(y_train_fold ~ poly(x_train_fold, degree))
    y_pred <- predict(model, newdata = data.frame(x_train_fold = x_val))
    errors[i] <- mean((y_val - y_pred)^2)
  }
  
  cv_errors[degree, "CV_Error"] <- mean(errors)
}

cv_errors
```

c. Plot the best model's fitted line in blue and compare to the true function (the red line from the previous plot). 

```{r}
degree_best <- which.min(cv_errors[, "CV_Error"])
best_model <- lm(train_y ~ poly(train_x, degree_best))

# Prediction
pred_x <- seq(min(x), max(x), length.out = 100)
pred_y <- predict(best_model, newdata = data.frame(train_x = pred_x))

{
  plot(x, noisy.y)
  lines(x, y, col = 'red')
  lines(pred_x, pred_y, col = 'blue')
}
```

d. Comment on the results of (c). Why was performance better or worse at different order polynomials?

The blue line represents the best model's fitted line, and the red line represents the true underlying function generating y. As we can see, the best model (in this case, with the degree that achieved the lowest CV error) has done a reasonable job of fitting the underlying trend in the data. Performance is better or worse at different order polynomials because low-degree polynomials may underfit the data (not capturing the complexity of the true function), while high-degree polynomials may overfit the data (capturing noise and not generalizing well).

e. Report the CV error and test error at each order of polynomial. Which achieves the lowest CV error? How does the CV error compare to the test error? Comment on the results.

```{r}
test_errors <- sapply(1:5, function(degree) {
  model <- lm(train_y ~ poly(train_x, degree))
  y_pred <- predict(model, newdata = data.frame(train_x = test_x))
  mean((test_y - y_pred)^2)
})

data.frame(Polynomial_Order = 1:5, CV_Error = cv_errors[, "CV_Error"], Test_Error = test_errors)
```

## 2. Classifying a toy dataset

a. Pick a new dataset from the `mlbench` package (one we haven't used in class that is 2-dimensional with two classes; Hint: run `ls(package:mlbench)`). Experiment with classifying the data using KNN at different values of k. Use cross-validation to choose your best model.

```{r}
# a. Pick a new dataset from the `mlbench` package
# Install mlbench package if not installed
if (!requireNamespace("mlbench", quietly = TRUE)) {
  install.packages("mlbench")
}

library(mlbench)

p<-mlbench.spirals(300)
Spirals <- data.frame(x_1 = p$x[, 1], x_2 = p$x[, 2], Class = as.factor(p$classes))
# Choose a 2-dimensional dataset with two classes
#data(Spirals)
head(Spirals)

# Prepare the data
set.seed(1)
train_idx <- sample(1:nrow(Spirals), size = floor(0.8 * nrow(Spirals)))
train_data <- Spirals[train_idx, ]
test_data <- Spirals[-train_idx, ]

# Load the 'class' package for KNN
if (!requireNamespace("class", quietly = TRUE)) {
  install.packages("class")
}
library(class)

# Experiment with classifying the data using KNN at different values of k
k_values <- 1:20
cv_errors <- numeric(length(k_values))

for (k in k_values) {
  folds <- cut(seq(1, nrow(train_data)), breaks = 10, labels = FALSE)
  
  errors <- numeric(10)
  for (i in 1:10) {
    fold_idx <- which(folds == i, arr.ind = TRUE)
    train_fold <- train_data[fold_idx, ]
    test_fold <- train_data[-fold_idx, ]
    
    knn_pred <- knn(train = test_fold[, 1:2], test = train_fold[, 1:2], cl = test_fold[, 3], k = k)
    errors[i] <- mean(train_fold[, 3] != knn_pred)
  }
  
  cv_errors[k] <- mean(errors)
}

# Find the best k based on CV errors
best_k <- which.min(cv_errors)
best_k

```

b. Plot misclassification error rate at different values of k.

```{r}
plot(k_values, cv_errors, type = "b", xlab = "k", ylab = "Misclassification Error Rate")
```

c. Plot the decision boundary for your classifier using the function at the top code block, `plot_decision_boundary()`. Make sure you load this function into memory before trying to use it.

```{r}
knn_pred_grid <- function(train_x, train_y, grid, k) {
  knn(train = train_x, test = grid, cl = train_y, k = k)
}

train_x <- as.matrix(train_data[, 1:2])
train_y <- as.factor(train_data[, 3])

grid_range <- expand.grid(x_1 = seq(min(train_x[, 1]), max(train_x[, 1]), length.out = 100),
                          x_2 = seq(min(train_x[, 2]), max(train_x[, 2]), length.out = 100))

pred_grid <- knn_pred_grid(train_x, train_y, grid_range, best_k)

plot_decision_boundary(train_x, train_y, pred_grid, grid_range)
```

## 3. Performance measures for classification

Recall the `Caravan` data from the week 2 lab (part of the `ISLR` package). Train a KNN model with k=2 using all the predictors in the dataset and the outcome `Purchase`. Create a confusion matrix with the test set predictions and the actual values of `Purchase`. Using the values of the confusion matrix, calculate precision, recall, and F1. (Note that `Yes` is the positive class and the confusion matrix may be differently oriented than the one presented in class.)

```{r}

# Load libraries
library(ISLR)
library(class)
library(caret)

# Load Caravan data
data(Caravan)

# Handle missing values
Caravan[is.na(Caravan)] <- 0

# Split data into predictors and outcome
X <- Caravan[, -86]
Y <- Caravan$Purchase

# Normalize the predictor variables
X_normalized <- as.data.frame(scale(X))

# Split the dataset into training and test sets
set.seed(123)
train_indices <- sample(1:nrow(Caravan), size = floor(0.7 * nrow(Caravan)))
X_train <- X_normalized[train_indices, ]
Y_train <- Y[train_indices]
X_test <- X_normalized[-train_indices, ]
Y_test <- Y[-train_indices]

# Train the KNN model with k=2
knn_pred <- knn(train = X_train, test = X_test, cl = Y_train, k = 2)

# Create confusion matrix
cm <- confusionMatrix(table(knn_pred, Y_test))

# Calculate precision, recall, and F1
precision <- cm$byClass['Pos Pred Value']
recall <- cm$byClass['Sensitivity']
F1 <- 2 * (precision * recall) / (precision + recall)

precision
recall
F1

```

```{r}
cm
```


## 4. ISLR Chapter 5 Exercise 3 (Optional Practice)

## 5. ISLR Chapter 5 Exercise 8 (Optional Practice)

a. We will now perform cross-validation on a simulated data set. In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form.

```{r}
set.seed(1)

x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
```
In this dataset, n is the number of observations and p is the number of predictors. Here, n = 100 and p = 1. The model used to generate the data is: $Y = X - 2X^2 + \varepsilon$


b. Create a scatterplot of $X$ against $Y$. Comment on what you find.

```{r}
plot(x, y, main = "Scatterplot of X against Y", xlab = "X", ylab = "Y")

```

The scatterplot shows a quadratic relationship between X and Y, as expected given the equation used to generate the data.


c. Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:

- $Y = \beta_0 + \beta_1X + \varepsilon$
- $Y = \beta_0 + \beta_1X + \beta_2X^2 + \varepsilon$
- $Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 + \varepsilon$
- $Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 +\beta_4X^4 + \varepsilon.$

Note you may find it helpful to use the \texttt{data.frame()} function
to create a single data set containing both $X$ and $Y$.

```{r}
library(boot)

# Create a single dataset containing both X and Y
data <- data.frame(x = x, y = y)

# Define a function to compute the LOOCV errors
loocv_error <- function(fit) {
  cv_error <- cv.glm(data, fit, K = nrow(data))$delta[1]
  return(cv_error)
}

set.seed(123)

# Fit the models
fit1 <- glm(y ~ x, data = data)
fit2 <- glm(y ~ x + I(x^2), data = data)
fit3 <- glm(y ~ x + I(x^2) + I(x^3), data = data)
fit4 <- glm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = data)

# Compute LOOCV errors
loocv_error1 <- loocv_error(fit1)
loocv_error2 <- loocv_error(fit2)
loocv_error3 <- loocv_error(fit3)
loocv_error4 <- loocv_error(fit4)

print(loocv_error1)
print(loocv_error2)
print(loocv_error3)
print(loocv_error4)

```

d. Repeat (c) using another random seed, and report your results.
Are your results the same as what you got in (c)? Why?

```{r}
set.seed(456)

# Fit the models
fit1 <- glm(y ~ x, data = data)
fit2 <- glm(y ~ x + I(x^2), data = data)
fit3 <- glm(y ~ x + I(x^2) + I(x^3), data = data)
fit4 <- glm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = data)

# Compute LOOCV errors
loocv_error1 <- loocv_error(fit1)
loocv_error2 <- loocv_error(fit2)
loocv_error3 <- loocv_error(fit3)
loocv_error4 <- loocv_error(fit4)

loocv_error1
loocv_error2
loocv_error3
loocv_error4
```
The results are the same as in (c) because LOOCV is deterministic and does not depend on the random seed.


e. Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.

```{r}
# Fit the models using least squares
model1 <- lm(y ~ x)
model2 <- lm(y ~ x + I(x^2))
model3 <- lm(y ~ x + I(x^2) + I(x^3))
model4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))

# Examine the statistical significance of the coefficient estimates
summary(model1)
summary(model2)
summary(model3)
summary(model4)


```

```{r}

```


